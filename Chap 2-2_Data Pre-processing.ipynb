{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyO6OHflcVCS+fpZUncEl9jQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jyPark1105/ML_DL_Study_Myself/blob/main/Chap%202-2_Data%20Pre-processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 2-2 : Data Pre-processing\n",
        "\n",
        "### Introduction\n",
        "\n",
        "        1. 올바른 결과 도출을 위해서 데이터를 사용하기 전 전처리 과정을 거침\n",
        "        2. 전처리 과정을 거친 데이터로 훈련했을 때의 차이를 알고, 표준점수로 특성의 스케일을 변환하는 방법을 배우기"
      ],
      "metadata": {
        "id": "uKS4fA4yazHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "z7WDqrQTRur4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/ML_DL_Alone')\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "ml_sr5SqRw5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Input data using Numpy"
      ],
      "metadata": {
        "id": "FAhwYVnA9eTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7,\n",
        "                31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5,\n",
        "                34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0,\n",
        "                38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 10.5, 10.6, 11.0, 11.2,\n",
        "                11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0]\n",
        "\n",
        "fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0,\n",
        "                450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0,\n",
        "                700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0,\n",
        "                700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0,\n",
        "                925.0, 975.0, 950.0, 6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0,\n",
        "                9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9]"
      ],
      "metadata": {
        "id": "clz0BlRddonK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### np.column_stack()\n",
        "\n",
        "**Tuple** : 원소에 순서가 있지만 한 번 만들어지면 수정할 수 없음\n",
        "\n",
        "    * 각각 Tuple 형태인 list1, list2를 Transpose하여 일렬로 세운 후, 'ㅡ'자로 리스트를 합쳐줌\n",
        "    * 또한 Numpy 배열로 출력하면 파이썬 list처럼 한 줄로 길게 출력되지 않고 matrix 형태로 출력됨\n"
      ],
      "metadata": {
        "id": "Zp9K1fooeSVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# 반드시 소괄호는 2개\n",
        "arr = np.column_stack(([1,2,3,4], [5,6,7,8]))\n",
        "print(arr)\n",
        "\n",
        "arr.shape"
      ],
      "metadata": {
        "id": "wwbcPNAhhV40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# column_stack() 함수를 사용하여 length와 weight 할당\n",
        "\n",
        "fish_data = np.column_stack((fish_length, fish_weight))\n",
        "\n",
        "# 0 ~ 4 idx까지 출력\n",
        "print(fish_data[:5])"
      ],
      "metadata": {
        "id": "pALynYXOhacg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Target data : np.ones(), np.zeros(), np.concatenate() function\n",
        "\n",
        "    마찬가지로 np.concatenate() 함수도 np.column_stack() 함수처럼 Tuple을 전달해야함"
      ],
      "metadata": {
        "id": "3kRWvXbVHH27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.ones(35))\n",
        "print(np.zeros(14))"
      ],
      "metadata": {
        "id": "bspbBfxlJdSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 타깃 데이터 연결하기\n",
        "fish_target = np.concatenate((np.ones(35), np.zeros(14)))\n",
        "\n",
        "print(fish_target)"
      ],
      "metadata": {
        "id": "KQJ4KlaeJ5kF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "    Numpy 라이브러리는 핵심 부분이 C, C++과 같은 저급 언어로 개발되어서 빠르고, Data Science 분야에 알맞게 최적화되어있음"
      ],
      "metadata": {
        "id": "BI4T8xlKKxfO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scikit-learn으로 Train set와 Test set 나누기\n",
        "\n",
        "**Scikit-learn** : 머신러닝 모델을 위한 알고리즘 뿐만 아니라 다양한 유틸리티 도구도 제공함\n",
        "\n",
        "    대표적인 도구 : model_selection 모듈 내의 train_test_split() 함수\n",
        "    * 전달되는 리스트 및 배열을 비율에 맞게 Train set과 Test set으로 나누어줌\n",
        "    * 또한 임의로 섞어주며 기본적으로 25%를 test set로 떼어냄"
      ],
      "metadata": {
        "id": "ymi_3jYMLZot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the train_test_split function\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# train_test_split 함수를 이용하여 data 분할하기\n",
        "train_input, test_input, train_target, test_target = train_test_split(\n",
        "    fish_data, fish_target, random_state=42)"
      ],
      "metadata": {
        "id": "R0P6IVJ5LwAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Numpy 배열의 shape 속성으로 입력의 크기를 출력\n",
        "print(train_input.shape, test_input.shape)\n",
        "print(train_target.shape, test_target.shape)\n"
      ],
      "metadata": {
        "id": "Q_Aa5OnvTUix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_target)"
      ],
      "metadata": {
        "id": "RrfNcvHPTnwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "    도미와 빙어의 비율이 약 3.3:1로 각각 10마리, 3마리로 Sampling Bias 현상이 일어남\n",
        "    \n",
        "    *이유: 전체 데이터셋 내에서 도미 개수 >> 빙어 개수, 무작위 샘플링을 취하더라도 샘플링 편향 현상이 일어날 수 있음\n",
        "\n",
        "    **해결법: stratify 매개변수에 타깃 데이터를 전달하면 클래스 비율에 맞게 데이터를 Split 해줌"
      ],
      "metadata": {
        "id": "xzc8uhW0TuPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_input, test_input, train_target, test_target = train_test_split(\n",
        "    fish_data, fish_target, stratify=fish_target, random_state=42)\n",
        "\n",
        "print(test_target)\n",
        "\n",
        "# 이 결과 꽤 비슷한 비율로 나눌 수 있음"
      ],
      "metadata": {
        "id": "OUtz0QMrUTcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 수상한 도미 한 마리\n",
        "\n",
        "    * 방금 준비한 데이터로 k-Nearest Neighbor 알고리즘을 훈련시키기\n",
        "\n",
        "    훈련 데이터 -> 모델 훈련\n",
        "    테스트 데이터 -> 모델 평가"
      ],
      "metadata": {
        "id": "mE108a-2UlZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "kn = KNeighborsClassifier()\n",
        "\n",
        "kn.fit(train_input, train_target)\n",
        "kn.score(test_input, test_target)"
      ],
      "metadata": {
        "id": "KGCaZzsvVAXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 김 팀장이 알려준 데이터 length=25, weight=150을 통해 inference\n",
        "print(kn.predict([[25, 150]]))\n",
        "\n",
        "# 도미(1)로 나와야할 데이터가 빙어(0)으로 나옴"
      ],
      "metadata": {
        "id": "8Q7cf44mV7af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.scatter(train_input[:,0], train_input[:,1])\n",
        "plt.scatter(25, 150, marker='^') # marker 매개변수를 모양을 지정\n",
        "plt.xlabel('length')\n",
        "plt.ylabel('weight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "REaoAoV8WIV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "    분명히 도미 데이터와 가까운 △ 데이터\n",
        "    그러나 KNN 모델은 0(빙어 데이터)이라 예측.\n",
        "\n",
        "    *KNeighborsClassifier 클래스는 주어진 샘플에서 가장 가까운 이웃을 찾아주는 kneighbors() method를 제공"
      ],
      "metadata": {
        "id": "qM_lK1LSWir2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### kneighbors method\n",
        "\n",
        "    1. 주어진 샘플에서 가장 가까운 이웃을 찾아줌\n",
        "    2. 이웃까지의 거리와 이웃 샘플의 인덱스를 반환"
      ],
      "metadata": {
        "id": "owMc5c20XDyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# n_neighbors=5이므로 5개씩 반환\n",
        "distances, indexes = kn.kneighbors([[25, 150]])\n",
        "\n",
        "print(distances)\n",
        "print(indexes)"
      ],
      "metadata": {
        "id": "qiViYXH3XcHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(train_input[:,0], train_input[:,1])\n",
        "plt.scatter(25, 150, marker='^') # marker 매개변수를 모양을 지정\n",
        "\n",
        "# indexes 배열을 사용하여 훈련 데이터 중 이웃 샘플을 따로 구분해 그리기. 'D' -> 마름모\n",
        "plt.scatter(train_input[indexes,0], train_input[indexes,1], marker='D')\n",
        "\n",
        "plt.xlabel('length')\n",
        "plt.ylabel('weight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "S86pvtxOXh4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 데이터 확인\n",
        "print(train_input[indexes])\n",
        "print(train_target[indexes])"
      ],
      "metadata": {
        "id": "lg-ihiZKYS-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "    length=25cm, weight=150g인 생선에 가장 가까운 이웃에는 빙어가 압도적으로 많아 빙어로 예측하는 것이 정상\n",
        "\n",
        "    distance 배열을 출력하여 이상한 결과를 판단하기"
      ],
      "metadata": {
        "id": "Dd6aBSQZY8-d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 기준 맞추기"
      ],
      "metadata": {
        "id": "DrelzAjHZUR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(distances)"
      ],
      "metadata": {
        "id": "kPqcATP8ZWgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "    distances[0]: (25,150) <-> (25.4, 242.0) 사이의 거리\n",
        "    distances[1]: (25,150) <-> (15.0, 19.9) 사이의 거리\n",
        "\n",
        "    그러나 scatter 함수를 통해 살펴보면 거리의 비율이 이상함\n",
        "\n",
        "    *이유: x축의 범위와 y축의 범위가 맞지않음\n",
        "\n",
        "    **해결법: plt.xlim() or ply.ylim() 함수 사용. 이 경우는 plt.xlim() 함수를 사용하는 것이 적절함"
      ],
      "metadata": {
        "id": "04idLFnEZZ0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(train_input[:,0], train_input[:,1])\n",
        "plt.scatter(25, 150, marker='^') # marker 매개변수를 모양을 지정\n",
        "\n",
        "# indexes 배열을 사용하여 훈련 데이터 중 이웃 샘플을 따로 구분해 그리기. 'D' -> 마름모\n",
        "plt.scatter(train_input[indexes,0], train_input[indexes,1], marker='D')\n",
        "\n",
        "# y range와 비율에 맞게 조정할 것\n",
        "plt.xlim(0, 1000)\n",
        "\n",
        "plt.xlabel('length')\n",
        "plt.ylabel('weight')\n",
        "plt.show()\n",
        "\n",
        "# 이 경우 x축은 크게 상관없고, y값의 편차가 좌우하는 것을 알려줌"
      ],
      "metadata": {
        "id": "DkY0oCmMZ9KY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Pre-processing\n",
        "\n",
        "    * Feature 사이에서 놓인 범위가 매우 다르다 -> \"Scale이 다르다\"\n",
        "    * Feature 간 Scale이 다른 일은 매우 흔하므로, 데이터를 표현하는 기준을 일정한 기준으로 맞춰주어야 알고리즘이 올바르게 예측할 수 있음\n",
        "    * 특히 이런 거리 기반의 알고리즘인 KNN 알고리즘에서 Scale을 더욱 신경써야함\n",
        "\n",
        "    이러한 작업을 데이터 전처리라 하며, 가장 널리 사용하는 전처리 방법들이 존재\n",
        "\n",
        "**Standard Score** (z score)\n",
        "\n",
        "1. Variance: 데이터에서 평균을 뺀 값을 모두 제곱 후 평균을 내어 구함\n",
        "2. Standard Deviation: 데이터가 분산된 정도를 나타내며 분산의 제곱근으로 구함\n",
        "3. Standard Score: 각 데이터가 원점에서 몇 표준편차만큼 떨어져 있는지 수치로 나타낸 값\n",
        "\n",
        "### np.mean(), np.std() 함수로 간단하게 구하기"
      ],
      "metadata": {
        "id": "plMOG9Siaim9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train_input.shape==(36, 2) 크기의 배열\n",
        "# 특성마다 값의 Scale이 다르므로 평균과 표준편차는 각 특성별로 계산해야하므로 이를 위해 axis=0(행) 지정 -> 행을 따라 각 열의 통계값을 계산\n",
        "mean = np.mean(train_input, axis=0)\n",
        "std = np.std(train_input, axis=0)\n",
        "\n",
        "print(mean, std)"
      ],
      "metadata": {
        "id": "ap6K6dPlbmqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 표준점수로 변환하기 위해 분산을 구하기\n",
        "    Numpy 라이브러리에서 train_input의 모든 행에서 mean에 있는 두 평균값을 빼고, std에 있는 두 표준편차를 모든 행에 적용시킴\n",
        "\n",
        "    이러한 기능 -> Broadcasting"
      ],
      "metadata": {
        "id": "SS1SFgzsdpOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 전체 훈련데이터에서 각각 평균값을 빼고, 표준편차로 나눠서 Scaling을 진행\n",
        "train_scaled = (train_input - mean) / std\n",
        "\n",
        "# 전처리된 훈련 데이터셋\n",
        "print(train_scaled)"
      ],
      "metadata": {
        "id": "Ms042VKie47v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 전처리된 데이터로 모델 훈련하기"
      ],
      "metadata": {
        "id": "mTMqf-Vme85u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 새로운 데이터 (25, 150)에 대해서도 동일한 전처리 작업 진행\n",
        "new = ([25, 150] - mean) / std\n",
        "\n",
        "# 표준점수로 변환한 train_scaled를 사용하여 산점도 그리기\n",
        "plt.scatter(train_scaled[:,0], train_scaled[:,1])\n",
        "plt.scatter(new[0], new[1], marker='^')\n",
        "plt.xlabel('length')\n",
        "plt.ylabel('weight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wJYLjpX8hcVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "    위의 실행: 표준편차로 변환하기 전의 산점도와 거의 동일한 형태\n",
        "\n",
        "    *달라진 점: x축과 y축의 범위(-1.5 ~ 1.5)\n",
        "    훈련 데이터의 두 feature가 비슷한 범위를 차지하고 있음"
      ],
      "metadata": {
        "id": "ZZ1ipGakFxBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kn.fit(train_scaled, train_target)\n",
        "\n",
        "# Test set도 전처리해주기\n",
        "test_scaled = (test_input - mean) / std\n",
        "\n",
        "kn.score(test_scaled, test_target)\n",
        "print(kn.predict([new]))"
      ],
      "metadata": {
        "id": "xZLeIgATHWa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "    kneighbors() 함수를 사용하여 new 샘플(25, 150)의 k-Neareast Neighbors를 구한 뒤 산점도로 그리기"
      ],
      "metadata": {
        "id": "-kqX5lwC04BL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distances, indexes = kn.kneighbors([new])\n",
        "plt.scatter(train_scaled[:,0], train_scaled[:,1])\n",
        "plt.scatter(new[0], new[1], marker='^')\n",
        "plt.scatter(train_scaled[indexes,0], train_scaled[indexes,1], marker='D')\n",
        "plt.xlabel('length')\n",
        "plt.ylabel('weight')\n",
        "plt.show()\n",
        "\n",
        "# 이로써 feature값의 scale에 민감하지 않고 안정적인 예측을 할 수 있음"
      ],
      "metadata": {
        "id": "V6Huu3kPHZpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 중간 정리\n",
        "\n",
        "    1. 혼공머신이 만든 모델은 완벽하게 테스트 세트를 분류하였지만, 김팀장이 가져온 new 샘플 (25, 150)에 대해서는 엉뚱하게 도미가 아닌 빙어로 예측함\n",
        "    2. 산점도로 그려보면 도미에 가깝지만, 2개의 feature인 length와 weight가 Scale이 다르기 때문에 일어난 일\n",
        "    3. 길이보다 무게의 크기에 따라 예측값이 좌지우지되었고, 이를 통해 머신러닝 알고리즘은 피쳐의 Scale이 다르면 잘 작동하지 않는다는 것을 알려줌\n",
        "    \n",
        "    *특성값 -> 표준점수로 변환\n",
        "    Scale 조정법은 표준점수가 기본적이고 가장 널리 사용되는 스케일링 방법\n",
        "\n",
        "    **데이터 전처리 시 주의사항\n",
        "    Train set를 변환한 방식을 Test set에도 동일하게 적용시켜야 함\n",
        "    (그렇지 않으면 엉뚱하게 변환될 것이고 훈련 세트로 훈련한 모델이 제대로 동작하지 않을 것)"
      ],
      "metadata": {
        "id": "kbnMKrV31cHu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 마무리\n",
        "\n",
        "1. Keywords\n",
        "    * **데이터 전처리** : 머신러닝 모델에 Train data를 주입하기 전 데이터를 가공하는 단계, 때로는 데이터 전처리에 많은 시간이 소모되기도 함\n",
        "\n",
        "    * 표준점수 : Train set의 scale을 바꾸는 대표적 방법 중 하나 ((Train data - mean) / std), 반드시 Train data의 mean과 std로 Test data를 scale 해야 함)\n",
        "\n",
        "    * Broadcasting : 크기가 다른 Numpy 배열에서 자동으로 사칙 연산을 모든 Row나 Column로 확장하여 수행하는 기능, train_scaled = (train_input - mean) / std\n",
        "\n",
        "\n",
        "2. 핵심 package와 function\n",
        "    * scikit-learn\n",
        "     * train_test_train(data, target, random_state=42) : Train data -> Train + Test로 나누는 함수\n",
        "     \n",
        "        1. test_size 매개변수를 통해 지정 가능. 기본값=0.25\n",
        "        2. shuffle 매개변수로 Train과 Test로 나누기 전 무작위로 섞을지 여부를 결정 가능. 기본값=True\n",
        "        3. stratify 매개변수에 class label이 담긴 배열(일반적으로 stratify=target data)을 전달하면 class의 ratio에 맞게 Train set와 Test set로 나눔\n",
        "\n",
        "     * kneighbors() : KNeighborsClassifier 클래스 내의 메서드 (입력한 데이터에 가장 가까운 이웃 개수를 찾아 distance와 index를 반환하며 기본적으로 클래의 객체 생성 시 n_neighbors=N에서 N개만큼의 가장 가까운 이웃들과의 거리와 인덱스들을 배열 형태로 반환\n",
        "     \n",
        "        return_distance=False로 지정하면 이웃 샘플의 index만 반환하고 distance는 반환하지 않음. 기본값=True"
      ],
      "metadata": {
        "id": "iuNe89kcHVv_"
      }
    }
  ]
}